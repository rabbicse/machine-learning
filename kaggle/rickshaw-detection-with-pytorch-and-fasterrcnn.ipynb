{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2991175,"sourceType":"datasetVersion","datasetId":1833016}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rabbi2k3/rickshaw-detection-with-pytorch-and-fasterrcnn?scriptVersionId=216471180\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nimport warnings\nfrom glob import glob\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nimport torchvision\nfrom PIL import Image\nfrom bs4 import BeautifulSoup\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.transforms import v2\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom skimage import io\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:45:55.287267Z","iopub.execute_input":"2024-12-04T10:45:55.287664Z","iopub.status.idle":"2024-12-04T10:45:55.293648Z","shell.execute_reply.started":"2024-12-04T10:45:55.287626Z","shell.execute_reply":"2024-12-04T10:45:55.292692Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Add utility functions","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\n\ndef get_model_instance_segmentation(num_classes, pretrained=True):\n    # load a model pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained)\n\n    # replace the classifier with a new one, that has\n    # num_classes which is user-defined\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    # model.train()\n\n    return model\n\n\ndef generate_box(obj):\n    x_min = int(obj.find('xmin').text)\n    y_min = int(obj.find('ymin').text)\n    x_max = int(obj.find('xmax').text)\n    y_max = int(obj.find('ymax').text)\n    return [x_min, y_min, x_max, y_max]\n\n\ndef generate_label(obj):\n    if 'rikshaw' in obj.find('name').text.lower():\n        return 1\n    return 0\n\n\ndef get_transform():\n    # Define the transform pipeline, including resizing\n    transform = transforms.Compose([        \n        transforms.ToTensor(),         # Convert image to tensor\n        transforms.RandomHorizontalFlip(p=0.5),\n        # transforms.ToDtype(torch.float32, scale=True),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n    ])\n    return transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:45:55.297771Z","iopub.execute_input":"2024-12-04T10:45:55.298462Z","iopub.status.idle":"2024-12-04T10:45:55.307368Z","shell.execute_reply.started":"2024-12-04T10:45:55.298436Z","shell.execute_reply":"2024-12-04T10:45:55.306565Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Create Rickshaw Dataset class","metadata":{}},{"cell_type":"code","source":"class RickshawDataset(Dataset):\n    def __init__(self, dataset_dir, transform=None, target_size=(112, 112)):\n        self.transform = transform\n        self.dataset_dir = dataset_dir\n        self.target_size = target_size\n        self.images = list(sorted(glob(os.path.join(self.dataset_dir, '*.jpg'))))\n        self.annotations = list(sorted(glob(os.path.join(self.dataset_dir, '*.xml'))))\n\n    def __getitem__(self, index):           \n        img_path = self.images[index]\n        # print(f\"image path: {img_path}\")\n\n        # Read image using OpenCV\n        img = cv2.imread(img_path)                   \n        original_width, original_height = img.shape[1], img.shape[0]  # (width, height)\n\n        img = cv2.resize(img, self.target_size)\n        \n        # Convert to RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        # Resize image        \n        img /= 255.0        \n\n        # transform Torchvision\n        resized_img = self.transform(img)\n\n        # Calculate scaling factors\n        scale_width = self.target_size[0] / original_width\n        scale_height = self.target_size[1] / original_height        \n\n        # Generate target and resize bounding boxes\n        target = self.__generate_target(index, self.annotations[index], (scale_width, scale_height))\n\n        del img\n\n        return resized_img, target\n\n    def __len__(self):\n        return len(self.images)\n\n    def __resize_box(self, box, original_size):\n        \"\"\"\n        Resize a bounding box according to the new image dimensions.\n        Args:\n            box (list): [x_min, y_min, x_max, y_max] bounding box.\n            original_size (tuple): (original_width, original_height) of the image.\n        Returns:\n            list: Resized bounding box.\n        \"\"\"\n        original_width, original_height = original_size\n        target_width, target_height = self.target_size\n\n        x_min, y_min, x_max, y_max = box\n        x_min = x_min * target_width / original_width\n        x_max = x_max * target_width / original_width\n        y_min = y_min * target_height / original_height\n        y_max = y_max * target_height / original_height\n\n        return [x_min, y_min, x_max, y_max]\n\n    # def __generate_target(self, image_id, file, original_size):\n    #     with open(file) as f:\n    #         data = f.read()\n    #         soup = BeautifulSoup(data, 'lxml')\n    #         objects = soup.find_all('object')\n\n    #         boxes = []\n    #         labels = []\n    #         for obj in objects:\n    #             box = generate_box(obj)\n    #             resized_box = self.__resize_box(box, original_size)\n    #             boxes.append(resized_box)\n    #             labels.append(generate_label(obj))\n\n    #         # Convert everything into a torch.Tensor\n    #         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n    #         labels = torch.as_tensor(labels, dtype=torch.int64)\n\n    #         img_id = torch.tensor([image_id])\n    #         area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n    #         iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n\n    #         # Annotation is in dictionary format\n    #         target = {\n    #             \"boxes\": boxes,\n    #             \"labels\": labels,\n    #             \"image_id\": img_id,\n    #             \"area\": area,\n    #             \"iscrowd\": iscrowd,\n    #         }\n    #         return target\n\n    def __generate_target(self, image_id, file, original_size):\n        with open(file) as f:\n            data = f.read()\n    \n        soup = BeautifulSoup(data, 'lxml')\n        objects = soup.find_all('object')\n    \n        # Use list comprehensions for efficiency\n        boxes = [self.__resize_box(generate_box(obj), original_size) for obj in objects]\n        labels = [generate_label(obj) for obj in objects]\n   \n        # Convert lists to tensors via NumPy arrays\n        boxes = torch.from_numpy(np.array(boxes, dtype=np.float32))       \n        labels = torch.from_numpy(np.array(labels, dtype=np.int64))\n    \n        # Constant tensors\n        img_id = torch.tensor([image_id])\n\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])       \n        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n    \n        # Annotation is in dictionary format\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": img_id,\n            \"area\": area,\n            \"iscrowd\": iscrowd,\n        }\n        # print(target)\n        return target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:45:55.309742Z","iopub.execute_input":"2024-12-04T10:45:55.309983Z","iopub.status.idle":"2024-12-04T10:45:55.325499Z","shell.execute_reply.started":"2024-12-04T10:45:55.30996Z","shell.execute_reply":"2024-12-04T10:45:55.324618Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"Create Rickshaw Train class ","metadata":{}},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:45:55.326797Z","iopub.execute_input":"2024-12-04T10:45:55.327042Z","iopub.status.idle":"2024-12-04T10:45:55.340489Z","shell.execute_reply.started":"2024-12-04T10:45:55.327018Z","shell.execute_reply":"2024-12-04T10:45:55.339676Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class TrainRickshaw:\n    def __init__(self):\n        self.train_iterator = None\n        self.valid_iterator = None\n        self.test_iterator = None\n\n        self.batch_size = 16\n\n        # Number of training epochs\n        self.num_epochs = 100\n\n        # Learning rate\n        self.lr = 0.0001\n\n        # Initiate net\n        model = get_model_instance_segmentation(2)\n        \n        # set device\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # # Wrap model in DataParallel for multi-GPU support\n        # if torch.cuda.device_count() > 1:\n        #     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n        #     device_ids = list(range(torch.cuda.device_count()))\n        #     # self.net = nn.DataParallel(model, device_ids = [0,1]) #.to(self.device)\n        #     self.net = nn.DataParallel(model, device_ids=device_ids)\n        # else:\n        #     self.net = model.to(self.device)\n\n        self.net = model.to(self.device)\n        # self.net = model\n        self.net.to(self.device)\n\n        # set optimizer\n        params = [p for p in self.net.parameters() if p.requires_grad]\n        self.optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.num_epochs)\n\n\n    def load_dataset(self):\n        dataset_dir = \"/kaggle/input/annotated-rickshaw-images-from-bangladesh/RIckshaw Data/\"\n        train_datasets = RickshawDataset(\n            dataset_dir=dataset_dir,\n            transform=get_transform())\n\n        self.train_iterator = DataLoader(dataset=train_datasets,\n                                         shuffle=True,\n                                         num_workers=8,\n                                         batch_size=self.batch_size,\n                                         collate_fn=collate_fn)\n\n        print('Load data done!')\n\n    def train_data(self):    \n        loss_hist = Averager()\n        for epoch in range(self.num_epochs):  # loop over the dataset multiple times\n            torch.cuda.empty_cache()\n            print(f'starting epoch: {epoch}')\n            print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n            print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n            print(f\"Current device: {torch.cuda.current_device()}\")\n            loss_hist.reset()\n            self.net.train()\n\n            i = 0\n            epoch_loss = 0\n            for imgs, annotations in self.train_iterator:\n                # print(f'Processing batch: {i} ...')\n                # print([img.shape for img in imgs])\n                i += 1\n\n                # batch_size = len(imgs)\n                # num_gpus = torch.cuda.device_count()\n                # assert batch_size % num_gpus == 0, \"Batch size must be divisible by the number of GPUs.\"\n                \n                # print([img.shape for img in imgs])  # Ensure all images have 3 channels\n                imgs = [img.to(self.device) for img in imgs]\n                # print([img.shape for img in imgs])  # Ensure all images have 3 channels\n                # imgs = list(img for img in imgs)\n                annotations = [{k: v.to(self.device) for k, v in t.items()} for t in annotations]\n                # annotations = [{k: v for k, v in t.items()} for t in annotations]\n\n                # print(f'Total images: {len(imgs)}')\n             \n                loss_dict = self.net(imgs, annotations)                \n                losses = sum(loss for loss in loss_dict.values())\n\n                loss_value = losses.item()\n                loss_hist.send(loss_value)\n\n                self.optimizer.zero_grad()\n                losses.backward()\n                self.optimizer.step()               \n\n            # print(f'Epoch: {epoch} Loss: {epoch_loss}')\n            print(f\"Epoch #{epoch} loss: {loss_hist.value}\")\n            self.lr_scheduler.step()\n\n        print('Finished Training')\n\n        # save model\n        model_path = os.path.abspath(\n            '/kaggle/working/rickshaw_net.pth')\n        torch.save(self.net.state_dict(), model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:45:55.40731Z","iopub.execute_input":"2024-12-04T10:45:55.407906Z","iopub.status.idle":"2024-12-04T10:45:55.418358Z","shell.execute_reply.started":"2024-12-04T10:45:55.40788Z","shell.execute_reply":"2024-12-04T10:45:55.417461Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"Run Train","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':   \n    trainer = TrainRickshaw()\n    trainer.load_dataset()\n    trainer.train_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T10:45:55.420784Z","iopub.execute_input":"2024-12-04T10:45:55.421111Z"}},"outputs":[{"name":"stdout","text":"Load data done!\nstarting epoch: 0\nIs CUDA available? True\nNumber of GPUs: 1\nCurrent device: 0\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}