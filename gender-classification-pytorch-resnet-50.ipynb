{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2476655,"sourceType":"datasetVersion","datasetId":1498843}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rabbi2k3/gender-classification-pytorch-resnet-50?scriptVersionId=216470981\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#/kaggle/input/biggest-genderface-recognition-dataset\nimport gc\nimport os\nimport time\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.models import resnet50\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nfrom torch.utils import data\nfrom torch.utils.data import random_split,Subset\nfrom torchvision import transforms\n\nwarnings.filterwarnings(\"ignore\")\n\n\nclass GenderNet(nn.Module):\n    def __init__(self):\n        super(GenderNet, self).__init__()\n        # Load pre-trained ResNet-50\n        self.base_model = resnet50(pretrained=True)\n\n        in_features = self.base_model.fc.in_features\n        self.base_model.fc = nn.Linear(in_features, 2)\n\n    def forward(self, x):\n        # Forward pass through ResNet-50\n        x = self.base_model(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T15:32:25.239151Z","iopub.execute_input":"2024-11-25T15:32:25.239663Z","iopub.status.idle":"2024-11-25T15:32:30.59785Z","shell.execute_reply.started":"2024-11-25T15:32:25.239611Z","shell.execute_reply":"2024-11-25T15:32:30.596986Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class GenderTrain:\n    def __init__(self):\n        self.root = os.path.abspath('/kaggle/input/biggest-genderface-recognition-dataset/faces/')\n        self.train_iterator = None\n        self.valid_iterator = None\n        self.test_iterator = None\n\n        # set hyper parameters\n        self.img_size = 112\n        self.means = (0, 0, 0)\n        self.stds = (1, 1, 1)\n\n        self.batch_size = 128\n\n        # Number of training epochs\n        self.num_epochs = 100\n\n        # Learning rate\n        self.lr = 0.0001\n\n        # Initiate net\n        self.net = GenderNet()\n\n        # set device\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.net.to(self.device)\n\n        # set optimizer\n        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n\n        # set criterion to calculate loss\n        self.criterion = nn.CrossEntropyLoss()\n        self.criterion.to(self.device)\n\n        # Learning Rate Scheduler\n        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.5)\n\n    def load_dataset(self):\n        transform_pipeline = transforms.Compose([\n            transforms.RandomResizedCrop(self.img_size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=self.means,\n                                 std=self.stds)\n        ])\n\n        full_dataset = torchvision.datasets.ImageFolder(root=self.root,\n                                                         transform=transform_pipeline)\n\n        # Split the dataset into train, validation, and test sets\n        total_size = len(full_dataset)\n        train_size = int(total_size * 0.7)\n        val_size = int(total_size * 0.2)\n        test_size = total_size - train_size - val_size  # Ensure all data is included\n\n        train_dataset, validation_dataset, test_dataset = random_split(\n            full_dataset, [train_size, val_size, test_size]\n        )\n\n\n        self.train_iterator = data.DataLoader(dataset=train_dataset,\n                                              shuffle=True,\n                                              num_workers=8,\n                                              batch_size=self.batch_size)\n\n        self.valid_iterator = data.DataLoader(dataset=validation_dataset,\n                                              shuffle=True,\n                                              num_workers=8,\n                                              batch_size=self.batch_size)\n\n        self.test_iterator = data.DataLoader(dataset=test_dataset,\n                                             shuffle=False,\n                                             num_workers=8,\n                                             batch_size=self.batch_size)\n\n        print('Load data done!')\n\n    def train(self, iterator):\n        # Local Parameters\n        epoch_loss = 0\n        epoch_acc = 0\n        start_time = time.time()\n\n        # Iterating over data loader\n        for images, labels in iterator:\n            # Loading images and labels to device\n            images = images.to(self.device)\n            labels = labels.to(self.device)\n\n            # Reseting Gradients\n            self.optimizer.zero_grad()\n\n            # Forward\n            outputs = self.net(images)\n\n            # identify max prediction\n            _, prediction = torch.max(outputs, 1)\n\n            # Calculating Loss\n            loss = self.criterion(outputs, labels)\n\n            # Backward\n            loss.backward()\n            self.optimizer.step()\n\n            # append losses\n            epoch_loss += loss.item() * images.size(0)\n\n            # append accuracy\n            epoch_acc += torch.sum(prediction == labels.data)\n\n            del images\n            del labels\n\n        # Overall Epoch Results\n        end_time = time.time()\n        total_time = end_time - start_time\n\n        # Acc and Loss\n        # epoch_loss = np.mean(epoch_loss)\n        # epoch_acc = np.mean(epoch_acc)\n        # self.lr_scheduler.step()\n        return epoch_loss / len(iterator.dataset), epoch_acc / len(iterator.dataset), total_time\n\n    def evaluate(self, iterator, best_val_acc, mode='test'):\n        # Local Parameters\n        epoch_loss = 0\n        epoch_acc = 0\n        start_time = time.time()\n\n        # Iterating over data loader\n        for images, labels in iterator:\n            # Loading images and labels to device\n            images = images.to(self.device)\n            labels = labels.to(self.device)\n\n            # Forward\n            outputs = self.net(images)\n\n            # identify max prediction\n            _, prediction = torch.max(outputs, 1)\n\n            # Calculating Loss\n            loss = self.criterion(outputs, labels)\n\n            # Calculate loss\n            epoch_loss += loss.item() * images.size(0)\n\n            # Calculating Accuracy\n            epoch_acc += torch.sum(prediction == labels.data)\n\n            del images\n            del labels\n\n        # Overall Epoch Results\n        end_time = time.time()\n        total_time = end_time - start_time\n\n        # Saving best model\n        if epoch_acc > best_val_acc and mode == 'val':\n            best_val_acc = epoch_acc\n            torch.save(self.net.state_dict(), os.path.abspath('/kaggle/working/resnet50_best.pth'))\n\n        return epoch_loss / len(iterator.dataset), epoch_acc / len(iterator.dataset), total_time, best_val_acc\n\n    def train_data(self):\n        best_val_acc = 0\n        torch.cuda.empty_cache()\n\n        for epoch in range(self.num_epochs):  # loop over the dataset multiple times\n            # Training\n            print(\"Training...\")\n            loss, acc, elapsed = self.train(self.train_iterator)\n            # Print Epoch Details\n            print(f'Epoch {epoch + 1} Loss : {loss} Acc : {(acc * 100): .3f} % Time : {elapsed}')\n\n            # Validation\n            print(\"Validating...\")\n            loss, acc, elapsed, best_val_acc = self.evaluate(self.valid_iterator, best_val_acc=best_val_acc, mode='val')\n            # Print Epoch Details\n            print(f'Epoch {epoch + 1} Loss : {loss} Acc : {(acc * 100):.3f} % Time : {elapsed}')\n\n            # Test\n            print(\"Testing...\")\n            loss, acc, elapsed, best_val_acc = self.evaluate(self.test_iterator, best_val_acc=best_val_acc)\n            # Print Epoch Details\n            print(f'Epoch {epoch + 1} Loss : {loss} Acc : Acc : {(acc * 100):.3f} % Time : {elapsed}')\n\n        print('Finished Training')\n\n        # save model\n        model_path = os.path.abspath('/kaggle/working/model.pth')\n        torch.save(self.net.state_dict(), model_path)\n\n\nif __name__ == '__main__':\n    trainer = GenderTrain()\n    trainer.load_dataset()\n    trainer.train_data()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T15:36:21.792251Z","iopub.execute_input":"2024-11-25T15:36:21.792626Z"}},"outputs":[{"name":"stdout","text":"Load data done!\nTraining...\nEpoch 1 Loss : 0.24715572638851682 Acc :  89.483 % Time : 58.86373543739319\nValidating...\nEpoch 1 Loss : 0.18719383764284522 Acc : 92.417 % Time : 6.935500383377075\nTesting...\nEpoch 1 Loss : 0.1966731342719354 Acc : Acc : 92.347 % Time : 3.991048574447632\nTraining...\nEpoch 2 Loss : 0.16566577794474616 Acc :  93.574 % Time : 65.4695417881012\nValidating...\nEpoch 2 Loss : 0.15552437428323995 Acc : 94.128 % Time : 6.805830717086792\nTesting...\nEpoch 2 Loss : 0.16098002506374787 Acc : Acc : 94.113 % Time : 4.13761830329895\nTraining...\nEpoch 3 Loss : 0.14580540622940674 Acc :  94.473 % Time : 65.35103011131287\nValidating...\nEpoch 3 Loss : 0.1407135162957132 Acc : 94.846 % Time : 6.9858644008636475\nTesting...\nEpoch 3 Loss : 0.1494502555181473 Acc : Acc : 94.334 % Time : 3.903759002685547\nTraining...\nEpoch 4 Loss : 0.13051697808025162 Acc :  95.020 % Time : 65.2849850654602\nValidating...\nEpoch 4 Loss : 0.1460079984558061 Acc : 94.515 % Time : 6.855999231338501\nTesting...\nEpoch 4 Loss : 0.14916152049286388 Acc : Acc : 94.150 % Time : 3.8884525299072266\nTraining...\nEpoch 5 Loss : 0.12377198801529121 Acc :  95.246 % Time : 64.91394472122192\nValidating...\nEpoch 5 Loss : 0.1366897540977103 Acc : 94.865 % Time : 7.14275860786438\nTesting...\nEpoch 5 Loss : 0.1416630662807741 Acc : Acc : 94.150 % Time : 3.8420016765594482\nTraining...\nEpoch 6 Loss : 0.11906418238786473 Acc :  95.446 % Time : 64.88121342658997\nValidating...\nEpoch 6 Loss : 0.13300718913173973 Acc : 94.902 % Time : 6.821225166320801\nTesting...\nEpoch 6 Loss : 0.14298584549588433 Acc : Acc : 94.371 % Time : 3.8891212940216064\nTraining...\nEpoch 7 Loss : 0.10987125964013278 Acc :  95.961 % Time : 65.00299954414368\nValidating...\nEpoch 7 Loss : 0.14514381090717687 Acc : 94.865 % Time : 6.804617881774902\nTesting...\nEpoch 7 Loss : 0.15141984907536055 Acc : Acc : 93.966 % Time : 4.178923606872559\nTraining...\nEpoch 8 Loss : 0.10528626575984522 Acc :  96.056 % Time : 64.99228525161743\nValidating...\nEpoch 8 Loss : 0.1466163559889631 Acc : 94.994 % Time : 6.921391248703003\nTesting...\nEpoch 8 Loss : 0.14670723324711485 Acc : Acc : 94.665 % Time : 3.9202239513397217\nTraining...\nEpoch 9 Loss : 0.09484613869538265 Acc :  96.429 % Time : 65.01694130897522\nValidating...\nEpoch 9 Loss : 0.14055551157655055 Acc : 94.883 % Time : 6.857980966567993\nTesting...\nEpoch 9 Loss : 0.1504409679645471 Acc : Acc : 94.702 % Time : 3.9248220920562744\nTraining...\nEpoch 10 Loss : 0.0953119011689095 Acc :  96.450 % Time : 64.62963199615479\nValidating...\nEpoch 10 Loss : 0.14599470046262988 Acc : 94.994 % Time : 7.147229433059692\nTesting...\nEpoch 10 Loss : 0.15827634267651525 Acc : Acc : 94.481 % Time : 3.992342233657837\nTraining...\nEpoch 11 Loss : 0.08943863275124338 Acc :  96.734 % Time : 64.96367979049683\nValidating...\nEpoch 11 Loss : 0.14316101991546631 Acc : 94.902 % Time : 6.837887763977051\nTesting...\nEpoch 11 Loss : 0.148425040693955 Acc : Acc : 94.592 % Time : 3.9128551483154297\nTraining...\nEpoch 12 Loss : 0.08693610293478037 Acc :  96.792 % Time : 64.86796259880066\nValidating...\nEpoch 12 Loss : 0.1485872541960708 Acc : 95.159 % Time : 6.827268362045288\nTesting...\nEpoch 12 Loss : 0.13511055871307018 Acc : Acc : 95.217 % Time : 4.00424599647522\nTraining...\nEpoch 13 Loss : 0.0875793556155003 Acc :  96.645 % Time : 64.5650315284729\nValidating...\nEpoch 13 Loss : 0.15309638454728466 Acc : 95.196 % Time : 8.258504867553711\nTesting...\nEpoch 13 Loss : 0.14472037353131417 Acc : Acc : 94.371 % Time : 4.428347110748291\nTraining...\nEpoch 14 Loss : 0.08631160071368071 Acc :  96.813 % Time : 64.99614453315735\nValidating...\nEpoch 14 Loss : 0.12952188182311583 Acc : 95.159 % Time : 6.817328929901123\nTesting...\nEpoch 14 Loss : 0.15443801724593853 Acc : Acc : 94.371 % Time : 3.9240000247955322\nTraining...\nEpoch 15 Loss : 0.0846545699330303 Acc :  96.634 % Time : 64.98476123809814\nValidating...\nEpoch 15 Loss : 0.1376457699294602 Acc : 95.859 % Time : 6.97478461265564\nTesting...\nEpoch 15 Loss : 0.1526468979623758 Acc : Acc : 94.960 % Time : 3.9892501831054688\nTraining...\nEpoch 16 Loss : 0.08273667595155308 Acc :  96.840 % Time : 65.13112020492554\nValidating...\nEpoch 16 Loss : 0.14225907616499664 Acc : 95.030 % Time : 7.044098138809204\nTesting...\nEpoch 16 Loss : 0.14038819837780714 Acc : Acc : 94.886 % Time : 3.925645112991333\nTraining...\nEpoch 17 Loss : 0.07847610975000154 Acc :  96.987 % Time : 65.02231097221375\nValidating...\nEpoch 17 Loss : 0.14259678331359235 Acc : 95.049 % Time : 6.89899468421936\nTesting...\nEpoch 17 Loss : 0.14549787746327075 Acc : Acc : 95.033 % Time : 3.9127469062805176\nTraining...\nEpoch 18 Loss : 0.07507735442528465 Acc :  97.018 % Time : 65.42952466011047\nValidating...\nEpoch 18 Loss : 0.15454787257273686 Acc : 95.178 % Time : 6.936681270599365\nTesting...\nEpoch 18 Loss : 0.146238631283079 Acc : Acc : 94.702 % Time : 3.983304262161255\nTraining...\nEpoch 19 Loss : 0.07131271718127115 Acc :  97.213 % Time : 65.02614736557007\nValidating...\nEpoch 19 Loss : 0.14624823453225375 Acc : 95.398 % Time : 6.838653326034546\nTesting...\nEpoch 19 Loss : 0.15746011449581915 Acc : Acc : 95.217 % Time : 3.8935954570770264\nTraining...\nEpoch 20 Loss : 0.06709478224432162 Acc :  97.450 % Time : 64.90345573425293\nValidating...\nEpoch 20 Loss : 0.14392988661195436 Acc : 95.454 % Time : 8.156836032867432\nTesting...\nEpoch 20 Loss : 0.14581047313190543 Acc : Acc : 95.070 % Time : 4.678860425949097\nTraining...\nEpoch 21 Loss : 0.07231471131190965 Acc :  97.218 % Time : 65.70897483825684\nValidating...\nEpoch 21 Loss : 0.15711574043174178 Acc : 95.196 % Time : 7.5664145946502686\nTesting...\nEpoch 21 Loss : 0.17160526361532996 Acc : Acc : 94.923 % Time : 4.620463848114014\nTraining...\nEpoch 22 Loss : 0.07049890091352018 Acc :  97.365 % Time : 64.83600664138794\nValidating...\nEpoch 22 Loss : 0.1557882353869148 Acc : 95.122 % Time : 6.896063566207886\nTesting...\nEpoch 22 Loss : 0.14990846081921247 Acc : Acc : 95.033 % Time : 4.069411993026733\nTraining...\nEpoch 23 Loss : 0.06548264534296887 Acc :  97.476 % Time : 64.4828405380249\nValidating...\nEpoch 23 Loss : 0.1545732989166732 Acc : 95.343 % Time : 6.8561601638793945\nTesting...\nEpoch 23 Loss : 0.16100464990421462 Acc : Acc : 94.849 % Time : 3.938051223754883\nTraining...\nEpoch 24 Loss : 0.06705663272962883 Acc :  97.297 % Time : 64.95395970344543\nValidating...\nEpoch 24 Loss : 0.17034487190681905 Acc : 95.049 % Time : 6.7958972454071045\nTesting...\nEpoch 24 Loss : 0.16392209249013395 Acc : Acc : 95.217 % Time : 3.9042868614196777\nTraining...\nEpoch 25 Loss : 0.06387211361388273 Acc :  97.549 % Time : 65.02316069602966\nValidating...\nEpoch 25 Loss : 0.1565172643480096 Acc : 95.030 % Time : 6.908775091171265\nTesting...\nEpoch 25 Loss : 0.17057025383382138 Acc : Acc : 94.886 % Time : 3.9330739974975586\nTraining...\nEpoch 26 Loss : 0.06278997523726587 Acc :  97.539 % Time : 64.7510118484497\nValidating...\nEpoch 26 Loss : 0.1446567677988568 Acc : 95.619 % Time : 6.96309232711792\nTesting...\nEpoch 26 Loss : 0.14194812877288718 Acc : Acc : 95.585 % Time : 3.922830581665039\nTraining...\nEpoch 27 Loss : 0.06241298410265723 Acc :  97.660 % Time : 64.66290402412415\nValidating...\nEpoch 27 Loss : 0.14024712106107723 Acc : 95.619 % Time : 6.752640008926392\nTesting...\nEpoch 27 Loss : 0.15606423136534894 Acc : Acc : 94.923 % Time : 3.926351308822632\nTraining...\nEpoch 28 Loss : 0.06493180316626448 Acc :  97.476 % Time : 64.72206926345825\nValidating...\nEpoch 28 Loss : 0.1456049123061377 Acc : 95.675 % Time : 7.278282642364502\nTesting...\nEpoch 28 Loss : 0.16326774586597903 Acc : Acc : 94.996 % Time : 3.8916242122650146\nTraining...\nEpoch 29 Loss : 0.06517353056638894 Acc :  97.407 % Time : 64.99977779388428\nValidating...\nEpoch 29 Loss : 0.1381571161172106 Acc : 95.343 % Time : 9.053351163864136\nTesting...\nEpoch 29 Loss : 0.16250948211407074 Acc : Acc : 95.033 % Time : 4.527248382568359\nTraining...\nEpoch 30 Loss : 0.05842341015606705 Acc :  97.660 % Time : 65.46514534950256\nValidating...\nEpoch 30 Loss : 0.15297769041128215 Acc : 95.638 % Time : 9.142306327819824\nTesting...\nEpoch 30 Loss : 0.16593964668737782 Acc : Acc : 95.180 % Time : 4.9091796875\nTraining...\nEpoch 31 Loss : 0.05986825186488104 Acc :  97.691 % Time : 65.78741478919983\nValidating...\nEpoch 31 Loss : 0.13676826969905384 Acc : 95.398 % Time : 6.892425298690796\nTesting...\nEpoch 31 Loss : 0.14404599232060494 Acc : Acc : 94.665 % Time : 4.138751268386841\nTraining...\nEpoch 32 Loss : 0.05866264784453945 Acc :  97.739 % Time : 64.78415775299072\nValidating...\nEpoch 32 Loss : 0.15631064447457238 Acc : 95.454 % Time : 7.131212949752808\nTesting...\nEpoch 32 Loss : 0.17944680224772408 Acc : Acc : 94.923 % Time : 4.225942134857178\nTraining...\nEpoch 33 Loss : 0.05913730535685991 Acc :  97.718 % Time : 64.89984512329102\nValidating...\nEpoch 33 Loss : 0.16930497375593076 Acc : 95.104 % Time : 6.990992546081543\nTesting...\nEpoch 33 Loss : 0.1973138904751356 Acc : Acc : 93.672 % Time : 3.9133331775665283\nTraining...\nEpoch 34 Loss : 0.05817865843613909 Acc :  97.723 % Time : 64.76582288742065\nValidating...\nEpoch 34 Loss : 0.14383957272002254 Acc : 95.656 % Time : 6.823016166687012\nTesting...\nEpoch 34 Loss : 0.1574264143821814 Acc : Acc : 94.996 % Time : 3.9965786933898926\nTraining...\nEpoch 35 Loss : 0.05888759309553679 Acc :  97.760 % Time : 64.72518563270569\nValidating...\nEpoch 35 Loss : 0.15287021567269174 Acc : 95.086 % Time : 7.1877264976501465\nTesting...\nEpoch 35 Loss : 0.16189300315900046 Acc : Acc : 94.886 % Time : 3.949843168258667\nTraining...\nEpoch 36 Loss : 0.057675905917490236 Acc :  97.707 % Time : 65.14514422416687\nValidating...\nEpoch 36 Loss : 0.14728175846528288 Acc : 95.546 % Time : 6.87876296043396\nTesting...\nEpoch 36 Loss : 0.16067389115373207 Acc : Acc : 95.033 % Time : 3.948683500289917\nTraining...\nEpoch 37 Loss : 0.058591153949881535 Acc :  97.770 % Time : 65.14346647262573\nValidating...\nEpoch 37 Loss : 0.15355599377720522 Acc : 95.767 % Time : 6.77965521812439\nTesting...\nEpoch 37 Loss : 0.17843812600880138 Acc : Acc : 94.886 % Time : 3.9377825260162354\nTraining...\nEpoch 38 Loss : 0.05587507693624366 Acc :  97.739 % Time : 64.94624495506287\nValidating...\nEpoch 38 Loss : 0.1663080134278017 Acc : 95.417 % Time : 6.836841344833374\nTesting...\nEpoch 38 Loss : 0.16023680248117517 Acc : Acc : 95.143 % Time : 3.925598621368408\nTraining...\nEpoch 39 Loss : 0.05378522862494932 Acc :  97.928 % Time : 64.69485545158386\nValidating...\nEpoch 39 Loss : 0.1516423873807676 Acc : 95.877 % Time : 6.779330492019653\nTesting...\nEpoch 39 Loss : 0.16626833702044902 Acc : Acc : 95.585 % Time : 3.989607810974121\nTraining...\nEpoch 40 Loss : 0.0557616919633291 Acc :  97.949 % Time : 64.77030634880066\nValidating...\nEpoch 40 Loss : 0.1526140003282897 Acc : 95.546 % Time : 6.782407283782959\nTesting...\nEpoch 40 Loss : 0.17889960090999132 Acc : Acc : 95.254 % Time : 3.910620927810669\nTraining...\nEpoch 41 Loss : 0.05713036432110014 Acc :  97.728 % Time : 64.75286102294922\nValidating...\nEpoch 41 Loss : 0.14848850014985168 Acc : 95.564 % Time : 6.866340398788452\nTesting...\nEpoch 41 Loss : 0.16885717376255216 Acc : Acc : 95.401 % Time : 3.871868371963501\nTraining...\nEpoch 42 Loss : 0.05477097626426012 Acc :  97.828 % Time : 64.87785911560059\nValidating...\nEpoch 42 Loss : 0.14722556875154347 Acc : 95.803 % Time : 7.029922008514404\nTesting...\nEpoch 42 Loss : 0.1469329950592266 Acc : Acc : 95.879 % Time : 3.8412506580352783\nTraining...\nEpoch 43 Loss : 0.05040704013390304 Acc :  97.991 % Time : 64.7799723148346\nValidating...\nEpoch 43 Loss : 0.16890644863900636 Acc : 95.711 % Time : 7.23379111289978\nTesting...\nEpoch 43 Loss : 0.16433667876182714 Acc : Acc : 95.401 % Time : 4.077754259109497\nTraining...\nEpoch 44 Loss : 0.05377171771913503 Acc :  97.818 % Time : 64.69980716705322\nValidating...\nEpoch 44 Loss : 0.1543408927627521 Acc : 95.895 % Time : 7.035029888153076\nTesting...\nEpoch 44 Loss : 0.16056224732297233 Acc : Acc : 95.291 % Time : 3.9256865978240967\nTraining...\nEpoch 45 Loss : 0.05417555225186354 Acc :  98.002 % Time : 64.76943683624268\nValidating...\nEpoch 45 Loss : 0.1490500128699253 Acc : 95.491 % Time : 6.931745767593384\nTesting...\nEpoch 45 Loss : 0.15702837517565887 Acc : Acc : 94.886 % Time : 4.032124996185303\nTraining...\nEpoch 46 Loss : 0.054060891866182445 Acc :  97.823 % Time : 64.6195113658905\nValidating...\nEpoch 46 Loss : 0.14870681321721874 Acc : 95.877 % Time : 6.904543399810791\nTesting...\nEpoch 46 Loss : 0.16750754665986328 Acc : Acc : 95.180 % Time : 3.972458600997925\nTraining...\nEpoch 47 Loss : 0.05319508802824327 Acc :  98.054 % Time : 64.51451587677002\nValidating...\nEpoch 47 Loss : 0.15105266532948636 Acc : 95.546 % Time : 6.821580648422241\nTesting...\nEpoch 47 Loss : 0.1659453044960041 Acc : Acc : 95.291 % Time : 3.901686191558838\nTraining...\nEpoch 48 Loss : 0.050095627712047446 Acc :  98.202 % Time : 65.03818798065186\nValidating...\nEpoch 48 Loss : 0.1415504822080666 Acc : 95.969 % Time : 7.872432231903076\nTesting...\nEpoch 48 Loss : 0.15823684667317284 Acc : Acc : 95.364 % Time : 4.21648383140564\nTraining...\nEpoch 49 Loss : 0.04951032675523742 Acc :  98.154 % Time : 64.68485283851624\nValidating...\nEpoch 49 Loss : 0.16012701667679316 Acc : 95.601 % Time : 6.84140944480896\nTesting...\nEpoch 49 Loss : 0.16154462877576012 Acc : Acc : 95.327 % Time : 3.9451346397399902\nTraining...\nEpoch 50 Loss : 0.0502796835529739 Acc :  98.086 % Time : 64.68576550483704\nValidating...\nEpoch 50 Loss : 0.14485929942297798 Acc : 96.006 % Time : 6.8351945877075195\nTesting...\nEpoch 50 Loss : 0.15129875222863828 Acc : Acc : 95.732 % Time : 3.9272897243499756\nTraining...\nEpoch 51 Loss : 0.047798746265862786 Acc :  98.233 % Time : 64.98435568809509\nValidating...\nEpoch 51 Loss : 0.1490592112500387 Acc : 96.024 % Time : 7.174805641174316\nTesting...\nEpoch 51 Loss : 0.18144356252521418 Acc : Acc : 94.849 % Time : 4.834070444107056\nTraining...\nEpoch 52 Loss : 0.04658870421096112 Acc :  98.112 % Time : 64.54873180389404\nValidating...\nEpoch 52 Loss : 0.1557579313646731 Acc : 96.024 % Time : 6.832150936126709\nTesting...\nEpoch 52 Loss : 0.18172873860887082 Acc : Acc : 95.217 % Time : 4.063161611557007\nTraining...\nEpoch 53 Loss : 0.04804552831053483 Acc :  98.086 % Time : 64.72494983673096\nValidating...\nEpoch 53 Loss : 0.1651647493777018 Acc : 95.417 % Time : 8.021971940994263\nTesting...\nEpoch 53 Loss : 0.17454471437478433 Acc : Acc : 94.996 % Time : 4.269784212112427\nTraining...\nEpoch 54 Loss : 0.04897764601600135 Acc :  98.007 % Time : 64.7917070388794\nValidating...\nEpoch 54 Loss : 0.1681256976692404 Acc : 95.325 % Time : 6.68751859664917\nTesting...\nEpoch 54 Loss : 0.16728961588081584 Acc : Acc : 95.659 % Time : 3.8992044925689697\nTraining...\nEpoch 55 Loss : 0.049749890990401696 Acc :  98.107 % Time : 64.66166186332703\nValidating...\nEpoch 55 Loss : 0.16307714514018817 Acc : 95.564 % Time : 6.8201634883880615\nTesting...\nEpoch 55 Loss : 0.1681993847673127 Acc : Acc : 95.291 % Time : 3.9319233894348145\nTraining...\nEpoch 56 Loss : 0.04693516023178085 Acc :  98.186 % Time : 65.03919386863708\nValidating...\nEpoch 56 Loss : 0.16615162238673392 Acc : 95.564 % Time : 6.955112457275391\nTesting...\nEpoch 56 Loss : 0.1835098898595417 Acc : Acc : 95.364 % Time : 4.02306604385376\nTraining...\nEpoch 57 Loss : 0.05055498742241041 Acc :  97.981 % Time : 64.61577701568604\nValidating...\nEpoch 57 Loss : 0.15944844870023278 Acc : 95.675 % Time : 6.8804404735565186\nTesting...\nEpoch 57 Loss : 0.17586503830834993 Acc : Acc : 95.291 % Time : 3.861001491546631\nTraining...\nEpoch 58 Loss : 0.04560483303618471 Acc :  98.170 % Time : 64.5209093093872\nValidating...\nEpoch 58 Loss : 0.17359257849561077 Acc : 95.472 % Time : 6.7758026123046875\nTesting...\nEpoch 58 Loss : 0.18540944101602783 Acc : Acc : 95.438 % Time : 4.184156656265259\nTraining...\nEpoch 59 Loss : 0.04648634234069524 Acc :  98.301 % Time : 64.65960741043091\nValidating...\nEpoch 59 Loss : 0.16220826023842294 Acc : 96.024 % Time : 7.06157922744751\nTesting...\nEpoch 59 Loss : 0.18006603096867416 Acc : Acc : 95.548 % Time : 3.9853503704071045\nTraining...\nEpoch 60 Loss : 0.04872416834972664 Acc :  98.086 % Time : 64.7035481929779\nValidating...\nEpoch 60 Loss : 0.1547910234293138 Acc : 95.693 % Time : 6.872416734695435\nTesting...\nEpoch 60 Loss : 0.16978769036416977 Acc : Acc : 95.254 % Time : 3.968839406967163\nTraining...\nEpoch 61 Loss : 0.049704626797344637 Acc :  98.075 % Time : 64.80948662757874\nValidating...\nEpoch 61 Loss : 0.1574435866957768 Acc : 95.454 % Time : 7.018493890762329\nTesting...\nEpoch 61 Loss : 0.16448750099916945 Acc : Acc : 95.180 % Time : 3.95784068107605\nTraining...\nEpoch 62 Loss : 0.046517836508295295 Acc :  98.117 % Time : 64.90158486366272\nValidating...\nEpoch 62 Loss : 0.1548585179715268 Acc : 95.822 % Time : 7.173109292984009\nTesting...\nEpoch 62 Loss : 0.16612615685764703 Acc : Acc : 95.585 % Time : 4.207178831100464\nTraining...\nEpoch 63 Loss : 0.043915540404652266 Acc :  98.249 % Time : 64.80348229408264\nValidating...\nEpoch 63 Loss : 0.18835123932956224 Acc : 95.159 % Time : 6.823460102081299\nTesting...\nEpoch 63 Loss : 0.209845032131764 Acc : Acc : 94.297 % Time : 4.809673547744751\nTraining...\nEpoch 64 Loss : 0.04621325035806358 Acc :  98.244 % Time : 65.17736458778381\nValidating...\nEpoch 64 Loss : 0.17594769793980342 Acc : 95.141 % Time : 7.190236806869507\nTesting...\nEpoch 64 Loss : 0.16191656787408965 Acc : Acc : 95.217 % Time : 4.6322736740112305\nTraining...\nEpoch 65 Loss : 0.04445001095702631 Acc :  98.207 % Time : 65.46992468833923\nValidating...\nEpoch 65 Loss : 0.16530369929749894 Acc : 95.362 % Time : 12.29730224609375\nTesting...\nEpoch 65 Loss : 0.18126950564302882 Acc : Acc : 95.033 % Time : 5.224341154098511\nTraining...\n","output_type":"stream"}],"execution_count":null}]}